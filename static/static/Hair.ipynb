{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import dlib\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from models.Embedding import Embedding\n",
    "from models.Alignment import Alignment\n",
    "from models.Blending import Blending\n",
    "\n",
    "from utils.drive import open_url\n",
    "from utils.shape_predictor import align_face\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_args = Namespace(unprocessed_dir = 'unprocessed', output_dir = 'input/face', output_size = 1024,\n",
    " cache_dir = 'cache', inter_method = 'bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shape Predictor\n"
     ]
    }
   ],
   "source": [
    "#Loading Shape Predictor for Alignment\n",
    "cache_dir = Path(align_args.cache_dir)\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_dir = Path(align_args.output_dir)\n",
    "output_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "print(\"Downloading Shape Predictor\")\n",
    "f=open_url(\"https://drive.google.com/uc?id=1huhv8PYpNNKbGCLOaYUjOgR1pY5pmbJx\", cache_dir=cache_dir, return_path=True)\n",
    "predictor = dlib.shape_predictor(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatic Image Cropper\n",
    "\n",
    "name: str = 'test'\n",
    "\n",
    "img = Image.open(f'unprocessed/{name}.jpg')\n",
    "\n",
    "img = img.crop((448, 28, 1472, 1052))\n",
    "img.save(f'unprocessed/{name}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maya.jpg: Number of faces detected: 1\n"
     ]
    }
   ],
   "source": [
    "def align_face(align_args):    \n",
    "    for im in Path(align_args.unprocessed_dir).glob(\"*.*\"):\n",
    "        faces = align_face(str(im),predictor)\n",
    "\n",
    "        for i,face in enumerate(faces):\n",
    "            if(align_args.output_size):\n",
    "                factor = 1024//align_args.output_size\n",
    "                assert align_args.output_size*factor == 1024\n",
    "                face_tensor = torchvision.transforms.ToTensor()(face).unsqueeze(0).cuda()\n",
    "                face_tensor_lr = face_tensor[0].cpu().detach().clamp(0, 1)\n",
    "                face = torchvision.transforms.ToPILImage()(face_tensor_lr)\n",
    "                if factor != 1:\n",
    "                    face = face.resize((align_args.output_size, align_args.output_size), Image.LANCZOS)\n",
    "            if len(faces) > 1:\n",
    "                face.save(Path(align_args.output_dir) / (im.stem+f\"_{i}.png\"))\n",
    "            else:\n",
    "                face.save(Path(align_args.output_dir) / (im.stem + f\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O arguments\n",
    "ref: str = 'image.png'\n",
    "tgt: str = f'{id}.png'\n",
    "\n",
    "args = Namespace(input_dir='input/face', output_dir='output', im_path1 = ref, im_path2=tgt, im_path3= tgt,\n",
    " sign='realistic', smooth=5, size=1024, ckpt='pretrained_models/ffhq.pt', channel_multiplier=2,\n",
    "  latent=512, n_mlp=8, device='cuda', seed=None, tile_latent=False, opt_name='adam',\n",
    "   learning_rate=0.01, lr_schedule='fixed', save_intermediate=False, save_interval=300,\n",
    "    verbose=False, seg_ckpt='pretrained_models/seg.pth', percept_lambda=1.0, l2_lambda=1.0,\n",
    "     p_norm_lambda=0.001, l_F_lambda=0.1, W_steps=250, FS_steps=250, ce_lambda=1.0, style_lambda=40000.0,\n",
    "      align_steps1=100, align_steps2=100, face_lambda=1.0, hair_lambda=1.0, blend_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path1 = os.path.join(args.input_dir, args.im_path1)\n",
    "im_path2 = os.path.join(args.input_dir, args.im_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN2 from checkpoint: pretrained_models/ffhq.pt\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /mnt/d/Programming/Spectrum/Barbershop/Barbershop/losses/lpips/weights/v0.1/vgg.pth\n",
      "...[net-lin [vgg]] initialized\n",
      "...Done\n",
      "Number of images: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:57<00:00, 57.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:48<00:00, 48.87s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Embed input image\n",
    "ii2s = Embedding(args)\n",
    "\n",
    "im_set = {im_path1}\n",
    "ii2s.invert_images_in_W([*im_set])\n",
    "ii2s.invert_images_in_FS([*im_set])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN2 from checkpoint: pretrained_models/ffhq.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Align Step 2:   0%|                                                                             | 0/100 [00:00<?, ?it/s]/mnt/d/Programming/miniconda3/envs/Barbershop/lib/python3.7/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "/mnt/d/Programming/miniconda3/envs/Barbershop/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "#Alignment Step\n",
    "\n",
    "align = Alignment(args)\n",
    "align.align_images(im_path1, im_path2, sign=args.sign, align_more_region=False, smooth=args.smooth)\n",
    "\n",
    "#originalimage_chosenimage.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN2 from checkpoint: pretrained_models/ffhq.pt\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /mnt/d/Programming/Spectrum/Barbershop/Barbershop/losses/masked_lpips/weights/v0.1/vgg.pth\n",
      "...[net-lin [vgg]] initialized\n",
      "...Done\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /mnt/d/Programming/Spectrum/Barbershop/Barbershop/losses/masked_lpips/weights/v0.1/vgg.pth\n",
      "...[net-lin [vgg]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "blend = Blending(args)\n",
    "blend.blend_images(im_path1, im_path2, im_path2, sign=args.sign)\n",
    "#originalimage_chosenimage_chosenimage.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
